The dogs vs cats dataset involves classifying photographs as either a dog or a cat. The training dataset consists of 25,000 images of dogs and cats. Of this there are 12,500 images of dogs and the same number of images as cats. The test set contains 12,500 images of dogs and cats. 

Step 1:

Let's first view the images of dogs and cats separately:

# plot dog photos from the dogs vs cats dataset 
from matplotlib import pyplot
from matplotlib.image import imread 
# define location of dataset
folder = 'train/train/' 
# plot first few images
for i in range(9): 
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # define filename
    filename = folder + 'dog.' + str(i) + '.jpg'
    # load image pixels
    image = imread(filename)
    # plot raw pixel data
    pyplot.imshow(image)
    # show the figure
    pyplot.show()

# plot cat photos from the dogs vs cats dataset 
from matplotlib import pyplot
from matplotlib.image import imread 
# define location of dataset
folder = 'train/train/' 
# plot first few images
for i in range(9): 
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # define filename
    filename = folder + 'cat.' + str(i) + '.jpg'
    # load image pixels
    image = imread(filename)
    # plot raw pixel data
    pyplot.imshow(image)
    # show the figure
    pyplot.show()

Step 2:

Organize the dataset into an appropriate structure:

- dataset_dogs_vs_cats
    -train
        -dogs
        -cats
    -test

# organize dataset into a useful structure
from os import makedirs
from os import listdir
from shutil import copyfile
from random import seed
from random import random
# create directories
dataset_home = 'dataset_dogs_vs_cats/'
subdirs = ['train/', 'test/']
for subdir in subdirs:
    # create label subdirectories
    labeldirs = ['dogs/', 'cats/']
    for labldir in labeldirs:
        newdir = dataset_home + subdir + labldir 
        makedirs(newdir, exist_ok=True)
        # seed random number
        generator seed(1)
        # define ratio of pictures to use for validation
        val_ratio = 0.25 
        # copy training dataset images into subdirectories
        src_directory = 'train/'
        for file in listdir(src_directory):
            src = src_directory + '/' + file
            dst_dir = 'train/'
            if random() < val_ratio:
                dst_dir = 'test/'
                if file.startswith('cat'):
                    dst = dataset_home + dst_dir + 'cats/' + file
                    copyfile(src, dst)
                    elif file.startswith('dog'):
                        dst = dataset_home + dst_dir + 'dogs/' + file 
                        copyfile(src, dst)

Step 3: Develop model - Here we use transfer learning

Transfer learning involves using all or parts of a model trained on a related task. Keras provides a range of pre-trained models that can be loaded and used wholly or partially via the Keras Applications API. A useful model for transfer learning is one of the VGG models, such as VGG-16 with 16 layers.

The model is comprised of two main parts, the feature extractor part of the model that is made up of VGG blocks, and the classiﬁer part of the model that is made up of fully connected layers and the output layer. 

We use the feature extraction part of the model and add a new classiﬁer part of the model that is tailored to the dogs and cats dataset. Speciﬁcally, we hold the weights of all of the convolutional layers ﬁxed during training, and only train new fully connected layers that will learn to interpret the features extracted from the model and make a binary classiﬁcation. This is achieved by loading the VGG-16 model, removing the fully connected layers from the output-end of the model, then adding the new fully connected layers to interpret the model output and make a prediction. The classiﬁer part of the model is removed automatically by setting the include top argument to False, which also requires that the shape of the input also be speciﬁed for the model, in this case (224, 224, 3). This means that the loaded model ends at the last max pooling layer, after which we manually add a Flatten layer and the new classiﬁer layers. The complete model is listed below.

import sys 
from matplotlib import pyplot
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
# define cnn model
def define_model():
    # load model 
    model = VGG16(include_top=False, input_shape=(224, 224, 3))
    # mark loaded layers as not trainable
    for layer in model.layers: 
        layer.trainable = False
        # add new classifier layers
        flat1 = Flatten()(model.layers[-1].output) 
        class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1) 
        output = Dense(1, activation='sigmoid')(class1) 
        # define new model 
        model = Model(inputs=model.inputs, outputs=output) 
        # compile model 
        opt = SGD(lr=0.001, momentum=0.9) 
        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
        return model
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss 
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['acc'], color='blue', label='train')
    pyplot.plot(history.history['val_acc'], color='orange', label='test')
    # save plot to file
filename = sys.argv[0].split('/')[-1] 
pyplot.savefig(filename + '_plot.png')
pyplot.close()
# run the test harness for evaluating a model
def run_test_harness():
    # define model
    model = define_model()
    # create data generator
    datagen = ImageDataGenerator(featurewise_center=True)
    # specify imagenet mean values for centering
    datagen.mean = [123.68, 116.779, 103.939]
    # prepare iterator
    train_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode='binary', batch_size=64, target_size=(224, 224)) 
    test_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode='binary', batch_size=64, target_size=(224, 224)) # fit model history = model.fit_generator(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=10, verbose=1) # evaluate model _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves 
    summarize_diagnostics(history)
    
    # entry point, run the test harness 
    run_test_harness()

output:

Found 18697 images belonging to 2 classes.
Found 6303 images belonging to 2 classes. 
> 97.636
Graphs of cross entropy loss and accuracy for the training and test datasets.

